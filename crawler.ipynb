{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/WKLeee/naver_news_crawler/blob/master/crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlEseFeN7MSr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fF9QcYjb7Umr"
   },
   "outputs": [],
   "source": [
    "# Search Options\n",
    "\n",
    "## Keyword\n",
    "keyword = '삼성전자'\n",
    "\n",
    "## Period\n",
    "start_date = '2020.09.28'\n",
    "end_date = '2020.09.28'\n",
    "\n",
    "## Sort Type (0 = Relevance, 1 = Latest Date, 2 = Oldest Date)\n",
    "sort_type = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "AKaSxXrFGY4Z",
    "outputId": "527a4613-1690-42e9-93b4-d8d1047ca397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&sm=tab_jum&query=삼성전자&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2020.09.28&de=2020.09.28\n"
     ]
    }
   ],
   "source": [
    "def naver_url_maker(keyword, start_date, end_date):\n",
    "  naver_url_query = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='\n",
    "  naver_url_sort = '&sm=tab_opt&sort='\n",
    "  naver_url_tail = '&photo=0&field=0&reporter_article=&pd=3&ds='\n",
    "  url = naver_url_query + keyword + naver_url_sort + sort_type + naver_url_tail + start_date + '&de=' + end_date\n",
    "  return url\n",
    "\n",
    "# Get Starting url\n",
    "current_url = naver_url_maker(keyword, start_date, end_date)\n",
    "print(current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EZa-K3F3GmAx",
    "outputId": "09093ff3-5b35-49c1-a521-ab884dd0d717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling... 44 articles...\n",
      "Crawling... 86 articles...\n",
      "Crawling... 124 articles...\n",
      "Crawling... 187 articles...\n",
      "Crawling... 237 articles...\n",
      "Crawling... 279 articles...\n",
      "Crawling... 298 articles...\n",
      "132.55396270751953\n"
     ]
    }
   ],
   "source": [
    "# Start Crawling\n",
    "start = time.time()\n",
    "file_num = 0\n",
    "pnum = 10 # The number of news pages to  save in one csv file\n",
    "anum = 0 # Index of Article\n",
    "csvfile = open('csvfile_'+str(file_num).zfill(3)+'.csv', 'w', encoding='utf-8-sig', newline='')\n",
    "wr = csv.writer(csvfile)\n",
    "\n",
    "page_num = 0\n",
    "\n",
    "update_url = current_url\n",
    "while True:\n",
    "  r = requests.get(update_url)\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "  link_element = soup.select('.txt_inline > a[href]')\n",
    "\n",
    "  for link in link_element:\n",
    "    naver_news_url = link['href'] # Naver News Url\n",
    "    rn = requests.get(naver_news_url)\n",
    "    soupn = BeautifulSoup(rn.text, 'html.parser')\n",
    "    head_tag = soupn.select('.article_info > h3')\n",
    "    if len(head_tag)>0:\n",
    "      article_header = head_tag[0].text # Article Title\n",
    "    body_tag = soupn.select('#articleBodyContents')\n",
    "    if len(body_tag)>0:\n",
    "      article_body = body_tag[0].text # Article Body\n",
    "    time_tag = soupn.select('.t11')\n",
    "    if len(time_tag)>0: # Article Time\n",
    "      article_time = time_tag[0].text\n",
    "    \n",
    "    wr.writerow([page_num, anum, article_time, article_header, article_body, naver_news_url])\n",
    "    anum += 1\n",
    "    \n",
    "  # If there is no next page, finish.\n",
    "  pages = soup.select('div.paging a.next')\n",
    "  if len(pages) == 0:\n",
    "    break\n",
    "  else:\n",
    "    href = pages[len(pages)-1]['href']\n",
    "    update_url = 'https://search.naver.com/search.naver'+href\n",
    "\n",
    "  page_num += 1\n",
    "  if page_num % pnum == 0: # One CSV file has the number of data\n",
    "    csvfile.close()\n",
    "    file_num += 1\n",
    "    csvfile = open('csvfile_'+str(file_num).zfill(3)+'.csv', 'w', encoding='utf-8-sig', newline='')\n",
    "    wr = csv.writer(csvfile)\n",
    "    print(\"Crawling...\",anum,\"articles...\")\n",
    "    \n",
    "csvfile.close()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPup1l6uomF9vVDlHJeaiiG",
   "include_colab_link": true,
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
